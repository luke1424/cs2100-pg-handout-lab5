Programming questions 2, 3, and 4 together comprise a small language model.
ChatGPT and Claude are large language models: they work like the model in this lab, but they use much more data and store much more information.

1. We suggested using the dataset of Olivia Rodrigo's song lyrics to train the small language model. How would it be different if we instead used a dataset of very serious legal documents? Or a dataset of your messages to your friends? How does the choice of dataset affect the output?
2. OpenAI has used various datasets to train the GPT models over the years. For example, GPT-1 was trained exclusively on a dataset made up of books, while GPT-3 was trained primarily on unfiltered text from the Internet. What are some tradeoffs between these sources of data? You may want to consider the outputs of the models, as well as any relevant privacy issues.
3. There are social biases encoded in any language model, which it learned from the dataset on which it was trained. How might they affect the output of the model?
