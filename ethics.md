Programming questions 2, 3, and 4 together comprise a small language model.
ChatGPT and Claude are large language models: they work like the model in this lab, but they use much more data and store much more information.

1. We suggested using the dataset of Olivia Rodrigo's song lyrics to train the small language model. How would it be different if we instead used a dataset of very serious legal documents? Or a dataset of your messages to your friends? How does the choice of dataset affect the output?
2. OpenAI has used various datasets to train the GPT models over the years. For example, GPT-1 was trained exclusively on a dataset made up of books, while GPT-3 was trained primarily on unfiltered text from the Internet. What are some tradeoffs between these sources of data? You may want to consider the outputs of the models, as well as any relevant privacy issues.
3. There are social biases encoded in any language model, which it learned from the dataset on which it was trained. How might they affect the output of the mode.
If the training set consists of song lyrics, the model will generate poetic and repetitive text; if it consists of legal documents, the output will be formal and complex; if it consists of conversations with friends, the output will be casual and relaxed. This shows that the dataset directly determines the style and use of the output. Books provide high-quality data but are limited in scope, making them suitable for training models with more standardized language; internet data covers a much broader range, but its quality is inconsistent and often noisy, though it may feel more “authentic.” Choosing a dataset therefore involves a tradeoff between breadth and quality, and it is also important to avoid violating privacy. If the training data contains social biases, then the language model will inevitably reflect them, and may therefore produce unfair or one-sided responses when people ask questions.